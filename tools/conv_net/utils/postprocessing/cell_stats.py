#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Nov 28 18:13:16 2018

@author: wanglab

by Tom Pisano (tpisano@princeton.edu, tjp7rr@gmail.com) & Zahra D (zmd@princeton.edu, zahra.dhanerawala@gmail.com)

"""

import numpy as np, cv2, sys, os, shutil
import time
from skimage.external import tifffile

from scipy import ndimage
from scipy.ndimage.morphology import generate_binary_structure
import pandas as pd


def consolidate_cell_measures(ignore_jobid_count = False, **params):
    """ consolidates cell measures jobs into one csv file """
    
    #grab csv output folder
    unet_output_dir = os.path.join(params["output_dir"], "3dunet_output")
    jobs = [xx for xx in os.listdir(unet_output_dir) if not xx == "pooled_cell_measures"]
    
    #check
    if len(jobs) == (int(params["inputshape"][0])/int(params["zsplt"]))+1 or ignore_jobid_count: #+1 to account for job number 0      
        list_of_single_dfs = [os.path.join(unet_output_dir, fl) for fl in jobs]; list_of_single_dfs.sort()
        #make folder for the pooled results
        if not os.path.exists(os.path.join(unet_output_dir, "pooled_cell_measures")): os.mkdir(os.path.join(unet_output_dir, "pooled_cell_measures"))
    
        #pool
        pooled_df = (pd.concat([pd.read_csv(fl) for fl in list_of_single_dfs], sort = True)).drop(columns="Unnamed: 0")
        pooled_df.to_csv(os.path.join(unet_output_dir, "pooled_cell_measures/{}_cell_measures.csv".format(params["expt_name"])))
        
        #move param dict
        shutil.move(os.path.join(params["data_dir"], "cnn_param_dict.csv"), 
                               os.path.join(os.path.join(params["output_dir"], "3dunet_output"), "cnn_param_dict.csv"))
        
        sys.stdout.write("\npooled cell measures for cell channel saved in: {}\n".format(os.path.join(unet_output_dir, "pooled_cell_measures/{}_cell_measures.csv".format(params["expt_name"]))))
    else:
        sys.stdout.write("\njobs have not completed for all z planes. check to see which jobs are missing in {}".format(unet_output_dir))
    
    sys.stdout.flush()


def calculate_cell_measures(**params):
    """ finds center of mass, sphericity, perimeter, and voxel count from cnn output """
    
    #calculate cell measures
    df = probabiltymap_to_cell_measures(params["reconstr_arr"], params["jobid"], threshold = params["threshold"], 
                                                            numZSlicesPerSplit = params["zsplt"], 
                                                            overlapping_planes = params["ovlp_plns"], cores = 1, 
                                                            verbose = params["verbose"])
    
    #make separate 3dunet output directory
    unet_output_dir = os.path.join(params["output_dir"], "3dunet_output")
    if not os.path.exists(unet_output_dir): os.mkdir(unet_output_dir)
    
    df.to_csv(os.path.join(unet_output_dir, "{}_jobid{}_cell_measures.csv".format(params["expt_name"], str(params["jobid"]).zfill(3))))
    
    #return csv path
    return os.path.join(unet_output_dir, "{}_jobid{}_cell_measures.csv".format(params["expt_name"], str(params["jobid"]).zfill(3)))


def probabiltymap_to_cell_measures(src, jobid, threshold = (0.6,1), numZSlicesPerSplit = 30, overlapping_planes = 30, cores = 1, 
                                    verbose = True, structure_rank_order = 2):
    """
    by tpisano
    
    Function to take probabilty maps generated by run_cnn and output centers based on center_of_mass, as well as sphericties and perimeter of cell

    Inputs:
    --------------
    memmapped_paths: (optional, str, or list of strs) pth(s) to memmapped array
    threshold: (tuple) lower and upper bounds to keep. e.g.: (0.1, 1) - note this assumes input from a CNN and thus data will
    numZSlicesPerSplit: chunk of zplanes to process at once. Adjust this and cores based on memory constraints.
    cores: number of parallel jobs to do at once. Adjust this and numZSlicesPerSplit based on memory constraints
    overlapping_planes: number of planes on each side to overlap by, this should be comfortably larger than the maximum z distances of a single object
    structure_rank_order: Optional. If true provides the structure element to used in ndimage.measurements.labels, 2 seems to be the most specific
    """
    #handle inputs
    if type(src) == str:
        if src[-3:] == "tif": src = tifffile.imread(src)
        if src[-3:] == "npy": src = np.lib.format.open_memmap(src, dtype = "float32", mode = "r")

    zdim, ydim, xdim = src.shape
    start = time.time()
    if verbose: sys.stdout.write("\n   thresholding, determining connected pixels, identifying center of masses\n\n")
    sys.stdout.flush() 
    
    iterlst=[(src, z, numZSlicesPerSplit, overlapping_planes, threshold, structure_rank_order) for z in range(0, zdim, numZSlicesPerSplit)]    
    if int(jobid) > len(iterlst):
         sys.stdout.write("\njobid > number of planes\n\n")
    else:
        #set chunk
        i = iterlst[int(jobid)]
        #run
        single_df = []
        single_df.append(find_labels_centerofmass_cell_measures(i[0], i[1], i[2], i[3], i[4], i[5]))
        if verbose: sys.stdout.write('\nfinished calculating cell measures for z planes: {}-{}\n'.format(i[1], i[1]+30)); sys.stdout.flush() 
        
        print ("total time {} minutes".format(round((time.time() - start) / 60)))
        
        #convert to pandas df
        single_df = pd.concat(single_df)
        return single_df

def find_labels_centerofmass_cell_measures(array, start, numZSlicesPerSplit, overlapping_planes, 
                                           threshold, structure_rank_order):
    """
    by tpisano
    
    """
    zdim, ydim, xdim = array.shape
    structure = generate_binary_structure(array.ndim, structure_rank_order) if structure_rank_order else None
    
    #get array
    if start==0:
        arr = array[:numZSlicesPerSplit+overlapping_planes]
    else:
        arr = array[max(start - overlapping_planes,0) : np.min(((start + numZSlicesPerSplit + overlapping_planes), zdim))]    
    #thresholding
    a = arr>=threshold[0]
    a = a.astype("bool") #ben - reduces size of arr 4 fold!
    
    #find labels
    labels = ndimage.measurements.label(a, structure)
    centers = ndimage.measurements.center_of_mass(a, labels[0], range(1, labels[1]+1))    
    #save to dataframe to use for contour mapping
    data = [[int(centers[i][0]), int(centers[i][1]), int(centers[i][2]), lbl] for i,lbl in enumerate(range(1, labels[1]+1))] #z,y,x,center
    df = pd.DataFrame(data, columns = ["z", "y", "x", "val"])
    
    #filter
    if start==0:
        #such that you only keep centers in first chunk - filter data frame
        df = df[df["z"] <= numZSlicesPerSplit]        
    else:
        #such that you only keep centers within middle third - filter data frame
        df = df[(df["z"] > (overlapping_planes)) & (df["z"] <= np.min(((numZSlicesPerSplit + overlapping_planes), zdim)))]                

    #modify the current dataframe - get perimeter of cell and sphericities
    if len(df) != 0:        
        df["p_s_z_v"] = df.apply(lambda row: perimeter_sphericity_voxels(bounding_box_from_center_array(labels[0], row["val"], (row["z"],row["y"], row["x"]))), 1)
    
        #now unpack
        df["maximum perimeter"] = df.apply(lambda row: row["p_s_z_v"][0],1)
        df["sphericity"] = df.apply(lambda row: row["p_s_z_v"][1],1)
        df["z depth"] = df.apply(lambda row: row["p_s_z_v"][2],1)
        df["no_voxels"] = df.apply(lambda row: row["p_s_z_v"][3],1)
        del df["p_s_z_v"]
        
        #test one that breaks
        #df = df.reset_index()
        #row = df.iloc[0]
        #perimeter_sphericity(bounding_box_from_center_array(labels[0], row["val"], (row["z"],row["y"], row["x"])))
        #ta = bounding_box_from_center_array(labels[0], row["val"], (row["z"],row["y"], row["x"]))
        #df[df["no_voxels"]>=6]["sphericity"].values <---this confirms that it"s how cv2 deals with very small objects, essentially less than 2 pixels in x or y
        
        #get intensities
        zyx_search_range = (5,10,10)
        df["intensity"] = df.apply(lambda row:helper_intensity(row["val"],row["x"],row["y"],row["z"], zyx_search_range, arr),1)
            
        #adjust z plane to accomodate chunkings
        if start!=0: df["z"] = df["z"]+(start-overlapping_planes) #only changing z based on z chunking
    
    return df

def perimeter_sphericity_voxels(src, dims = 3):
    """
    src = 3d
    
    looks at it from two perspectives and then takes the total average
    dims=(2,3) number of dimensions to look at
   
    sometime two contours are found on a zplane after labels - in this case take min, but could take average?
    """
    #initialise
    sphericities = []; perimeters = []
    
    if 0 in src.shape: 
        return 0, 0 #projects against empty labels    
    else:
        no_voxels = len(np.nonzero(src)[0])
        #in z dimension
        for z in src:
            try:
                contours = findContours(z.astype("uint8"))
                circ = circularity(contours)
                perimeter = cv2.arcLength(contours, True)
                sphericities.append(circ); perimeters.append(perimeter)
            except:
                "no cell in plane"
                
        #look in y dimension
        if dims == 3:
            for z in np.swapaxes(src, 0, 1):
                try:
                    contours = findContours(z.astype("uint8"))
                    circ = circularity(contours)
                    perimeter = cv2.arcLength(contours, True)
                    sphericities.append(circ); perimeters.append(perimeter)
                except:
                    "no cell in plane"
        
        #return - maximum perimeter and mean sphericity per cell
        if len(perimeters) > 0:
            perimeter = np.max(perimeters)
            zspan = len(perimeters)
        else:
            perimeter = perimeters
            zspan = None #if contour found no cell in z planes
        if len(sphericities) > 0: 
            sphericity = np.mean(sphericities) 
        else: 
            sphericity = sphericities
            
        #return values
        return perimeter, sphericity, zspan, no_voxels


def circularity(contours):
    """
    A Hu moment invariant as a shape circularity measure, Zunic et al, 2010
    """
    #moments = [cv2.moments(c.astype(float)) for c in contours]
    #circ = np.array([(m["m00"]**2)/(2*np.pi*(m["mu20"]+m["mu02"])) if m["mu20"] or m["mu02"] else 0 for m in moments])
    circ = [ (4*np.pi*cv2.contourArea(c))/(cv2.arcLength(c,True)**2) for c in contours]

    return np.asarray(circ)

def findContours(z):
    
    contours,hierarchy = cv2.findContours(z, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #if need more than two values to unpack error here upgrade to cv2 3+
    
    contours = np.asarray([c.squeeze() for c in contours if cv2.contourArea(c)>0])
    
    return contours

def bounding_box_from_center_array(src, val, center, box_size=(30,75,75)):
    """ faster version of _array to grab cell around center """
    
    z,y,x = [int(xx) for xx in center]
    zr, yr, xr = box_size
    
    out = src[max(0,z-box_size[0]):z+box_size[0], max(0, y-box_size[1]):y+box_size[1], max(x-box_size[2],0):x+box_size[2]] #copy is critical   
    #convert to boolean
    a = (out==val).astype(int)
    
    return a
    
def helper_intensity(val,x,y,z, zyx_search_range, cnn_src):
    """mini function to utilize pandas parallelization
    """
    #setting the proper ranges
    zr,yr,xr = zyx_search_range
    
    #making sure ranges are not negative
    rn = []
    for xx,yy in zip((z,y,x), zyx_search_range):
        if yy < xx:
            rn.append((int(xx-yy), int(xx+yy+1)))
        else:
            rn.append((int(xx), int(xx+yy+1)))

    #find the maximum part of cell 
    return np.max(cnn_src[rn[0][0]:rn[0][1], rn[1][0]:rn[1][1], rn[2][0]:rn[2][1]])
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial to setup parameter dictionary for large-scale inference using the 3D U-Net.\n",
    "Assumes that training, evaluation, and stitching and blending of images is completed through this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import os, sys, shutil\n",
    "#navigate one folder back up to be in the main repo - can do this a number of ways\n",
    "os.chdir(\"/jukebox/wang/zahra/python/lightsheet_py3\")\n",
    "print(os.getcwd())\n",
    "import argparse   \n",
    "from tools.utils.io import load_kwargs\n",
    "from tools.conv_net.utils.preprocessing.preprocess import get_dims_from_folder, make_indices, make_memmap_from_tiff_list, generate_patch, reconstruct_memmap_array_from_tif_dir\n",
    "from tools.conv_net.utils.postprocessing.cell_stats import calculate_cell_measures, consolidate_cell_measures\n",
    "from tools.conv_net.utils.preprocessing.check import check_patchlist_length_equals_patches    \n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup main run function for CPU-based pre- and post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**args):\n",
    "    \n",
    "    #args should be the info you need to specify the params\n",
    "    # for a given experiment, but only params should be used below\n",
    "    params = fill_params(**args)\n",
    "    \n",
    "    if params[\"stepid\"] == 0:\n",
    "        #######################################PRE-PROCESSING FOR CNN INPUT --> MAKING INPUT ARRAY######################################################\n",
    "        \n",
    "        #make directory to store patches\n",
    "        if not os.path.exists(params[\"data_dir\"]): os.mkdir(params[\"data_dir\"])\n",
    "    \t#save params to .csv file\n",
    "        save_params(params, params[\"data_dir\"])\n",
    "        \n",
    "        #convert full size data folder into memmap array\n",
    "        make_memmap_from_tiff_list(params[\"cellch_dir\"], params[\"data_dir\"], \n",
    "                                               params[\"cores\"], params[\"dtype\"], params[\"verbose\"])\n",
    "            \n",
    "    elif params[\"stepid\"] == 1:\n",
    "        #######################################PRE-PROCESSING FOR CNN INPUT --> PATCHING###################################################\n",
    "        \n",
    "        #generate memmap array of patches\n",
    "        patch_dst = generate_patch(**params)\n",
    "        sys.stdout.write(\"\\nmade patches in {}\\n\".format(patch_dst)); sys.stdout.flush()\n",
    "        \n",
    "    elif params[\"stepid\"] == 11:\n",
    "        #######################################CHECK TO SEE WHETHER PATCHING WAS SUCCESSFUL###################################################\n",
    "        \n",
    "        #run checker\n",
    "        check_patchlist_length_equals_patches(**params)\n",
    "        sys.stdout.write(\"\\nready for inference!\"); sys.stdout.flush()\n",
    "\n",
    "    elif params[\"stepid\"] == 21:\n",
    "        ####################################POST CNN --> INITIALISING RECONSTRUCTED ARRAY FOR ARRAY JOB####################################\n",
    "        \n",
    "        sys.stdout.write(\"\\ninitialising reconstructed array...\\n\"); sys.stdout.flush()\n",
    "        np.lib.format.open_memmap(params[\"reconstr_arr\"], mode=\"w+\", shape = params[\"inputshape\"], dtype = params[\"dtype\"])\n",
    "        sys.stdout.write(\"done :]\\n\"); sys.stdout.flush()\n",
    "\n",
    "    elif params[\"stepid\"] == 2:\n",
    "        #####################################POST CNN --> RECONSTRUCTION AFTER RUNNING INFERENCE ON TIGER2#################################\n",
    "        \n",
    "        #reconstruct\n",
    "        sys.stdout.write(\"\\nstarting reconstruction...\\n\"); sys.stdout.flush()\n",
    "        reconstruct_memmap_array_from_tif_dir(**params)\n",
    "        if params[\"cleanup\"]: shutil.rmtree(params[\"cnn_dir\"])\n",
    "\n",
    "    elif params[\"stepid\"] == 3:\n",
    "        ##############################################POST CNN --> FINDING CELL CENTERS#####################################################   \n",
    "        \n",
    "        save_params(params, params[\"data_dir\"])\n",
    "        \n",
    "        #find cell centers, measure sphericity, perimeter, and z span of a cell\n",
    "        csv_dst = calculate_cell_measures(**params)\n",
    "        sys.stdout.write(\"\\ncell coordinates and measures saved in {}\\n\".format(csv_dst)); sys.stdout.flush()\n",
    "        \n",
    "    elif params[\"stepid\"] == 4:\n",
    "        ##################################POST CNN --> CONSOLIDATE CELL CENTERS FROM ARRAY JOB##############################################\n",
    "        \n",
    "        #part 1 - check to make sure all jobs that needed to run have completed; part 2 - make pooled results\n",
    "        consolidate_cell_measures(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter dictionary setup function.\n",
    "For the pre-processing parameters:\n",
    "1. \"float32\" is the recommeded data type for `params[\"dtype\"]`.\n",
    "2. Modify `params[\"cores\"]` accordingful to local desktop or cluster capabilities (6 or higher recommeded).\n",
    "3. `params[\"cleanup\"]` boolean deletes memmory mapped arrays after patches have been generated. Recommend `False` for testing.\n",
    "4. `params[\"patchsz\"]` is entirely dependent on the image dimensions. Recommend 60 planes in the z dimension and dividing the `x` and `y` dimensions by 2 and adding 32 for best performance for 4x or tiled LBVT data and adding 32 to the `x` and `y` dimensions for single-tiled images.\n",
    "5. Specify window used for inference in `params[\"window\"]`. This does not get used but is saved to the parameter dictionary copy generated in the lightsheet data directory.\n",
    "\n",
    "The post-processing parameters can also be changed after pre-processing. Note:\n",
    "1. `params[\"threshold\"]` is important; it is the threshold by which cells will be segmented and centers generated. This can be determined by sweeping for the threshold in your validation data during training and picking the threshold with the best performance.\n",
    "2. `params[\"zsplt\"]` and `params[\"ovlp_plns\"]` relates to the number of planes considered at a time for 3D connected component analysis. Recommend 30 for both for 4x LBVT cellular resolution data for memory efficiency and best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_params(expt_name, stepid, jobid):\n",
    "\n",
    "    params = {}\n",
    "\n",
    "    #slurm params\n",
    "    params[\"stepid\"]        = stepid\n",
    "    params[\"jobid\"]         = jobid \n",
    "    \n",
    "    #experiment params\n",
    "    params[\"expt_name\"]     = os.path.basename(os.path.abspath(os.path.dirname(expt_name))) #going one folder up to get to fullsizedata\n",
    "        \n",
    "    #find cell channel tiff directory from parameter dict\n",
    "    kwargs = load_kwargs(os.path.dirname(expt_name))\n",
    "    print(\"\\n\\n loading params for: {}\".format(os.path.dirname(expt_name)))\n",
    "    vol = [vol for vol in kwargs[\"volumes\"] if vol.ch_type == \"cellch\"][0]\n",
    "    src = vol.full_sizedatafld_vol\n",
    "    assert os.path.isdir(src), \"nonexistent data directory\"\n",
    "    print(\"\\n\\n data directory: {}\".format(src))\n",
    "    \n",
    "    params[\"cellch_dir\"]    = src\n",
    "    params[\"scratch_dir\"]   = \"/jukebox/scratch/zmd\" #whatever path you are saving patches and reconstructed arrays to to\n",
    "    params[\"data_dir\"]      = os.path.join(params[\"scratch_dir\"], params[\"expt_name\"])\n",
    "    \n",
    "    #changed paths after cnn run\n",
    "    params[\"cnn_data_dir\"]  = os.path.join(params[\"scratch_dir\"], params[\"expt_name\"])\n",
    "    params[\"cnn_dir\"]       = os.path.join(params[\"cnn_data_dir\"], \"output_chnks\") #set cnn patch directory\n",
    "    params[\"reconstr_arr\"]  = os.path.join(params[\"cnn_data_dir\"], \"reconstructed_array.npy\")\n",
    "    params[\"output_dir\"]    = expt_name\n",
    "    \n",
    "    #pre-processing params\n",
    "    params[\"dtype\"]         = \"float32\"\n",
    "    params[\"cores\"]         = 8\n",
    "    params[\"verbose\"]       = True\n",
    "    params[\"cleanup\"]       = False\n",
    "    \n",
    "    params[\"patchsz\"]       = (60, 3840, 3328) #cnn window size for lightsheet = typically 20, 192, 192 for 4x, 20, 32, 32 for 1.3x\n",
    "    params[\"stridesz\"]      = (40, 3648, 3136) \n",
    "    params[\"window\"]        = (20, 192, 192)\n",
    "    \n",
    "    params[\"inputshape\"]    = get_dims_from_folder(src)\n",
    "    params[\"patchlist\"]     = make_indices(params[\"inputshape\"], params[\"stridesz\"])\n",
    "    \n",
    "    #post-processing params\n",
    "    params[\"threshold\"]     = (0.45,1) #h129 = 0.6; prv = 0.48\n",
    "    params[\"zsplt\"]         = 30\n",
    "    params[\"ovlp_plns\"]     = 30\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pre- and post-processing the data for cell detection, the code saves out the parameters you picked per sample as a `.csv` in the `3dunet_output` folder in the main data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params, dst):\n",
    "    \"\"\" \n",
    "    save params in cnn specific parameter dictionary for reconstruction/postprocessing \n",
    "    can discard later if need be\n",
    "    \"\"\"\n",
    "    (pd.DataFrame.from_dict(data=params, orient=\"index\").to_csv(os.path.join(dst, \"cnn_param_dict.csv\"),\n",
    "                            header = False))\n",
    "    sys.stdout.write(\"\\nparameters saved in: {}\".format(os.path.join(dst, \"cnn_param_dict.csv\"))); sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run locally for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #init\n",
    "    args = {}\n",
    "    \n",
    "    args[\"stepid\"] = 0 #Step ID to run patching, reconstructing, or cell counting\n",
    "    args[\"jobid\"] = 0 #Job ID to run as an array job, useful only for step 1,2,3; see slurm files for more info\n",
    "    args[\"expt_name\"] = \"/jukebox/wang/pisano/tracing_output/antero_4x/20160823_tp_bl6_cri_500r_02/lightsheet\" #Tracing output directory (aka registration output)\n",
    "    \n",
    "    main(**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
